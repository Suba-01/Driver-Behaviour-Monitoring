{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52a6e9cc-3789-437b-8ba8-12984acc5a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning for available cameras...\n",
      "Camera found at index 0\n",
      "Camera found at index 1\n",
      "Camera found at index 2\n",
      "Camera found at index 3\n",
      "Camera scan complete.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "print(\"Scanning for available cameras...\")\n",
    "\n",
    "for i in range(10):\n",
    "    cap = cv2.VideoCapture(i)\n",
    "    if cap.isOpened():\n",
    "        print(f\"Camera found at index {i}\")\n",
    "        cap.release()\n",
    "\n",
    "print(\"Camera scan complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "073a2857-261c-4170-9445-e1eb6fc0c025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Sanjay\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to C:\\Users\\Sanjay/.cache\\torch\\hub\\master.zip\n",
      "YOLOv5  2025-3-10 Python-3.10.0 torch-2.6.0+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "import winsound\n",
    "import torch\n",
    "from scipy.spatial import distance as dist\n",
    "from imutils import face_utils\n",
    "from deepface import DeepFace\n",
    "\n",
    "# Function to play alarm sound\n",
    "def sound_alarm():\n",
    "    winsound.Beep(1000, 1000)\n",
    "\n",
    "# Eye Aspect Ratio (EAR) function for drowsiness detection\n",
    "def eye_aspect_ratio(eye):\n",
    "    A = dist.euclidean(eye[1], eye[5])  \n",
    "    B = dist.euclidean(eye[2], eye[4])  \n",
    "    C = dist.euclidean(eye[0], eye[3])  \n",
    "    return (A + B) / (2.0 * C)\n",
    "\n",
    "# Drowsiness detection constants\n",
    "EYE_AR_THRESH = 0.3\n",
    "EYE_AR_CONSEC_FRAMES = 48 \n",
    "\n",
    "# Load dlib's face detector and facial landmark predictor\n",
    "shape_predictor_path = r'C:\\Users\\Sanjay\\AppData\\Roaming\\Microsoft\\Windows\\Start Menu\\Programs\\Python 3.12\\Project\\shape_predictor_68_face_landmarks.dat'\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(shape_predictor_path)\n",
    "\n",
    "(lStart, lEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"left_eye\"]\n",
    "(rStart, rEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"right_eye\"]\n",
    "\n",
    "# Load YOLOv5 model for pedestrian detection\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', force_reload=True, trust_repo=True)\n",
    "\n",
    "# Initialize cameras\n",
    "cap_driver = cv2.VideoCapture(1)  # Built-in camera for driver monitoring\n",
    "cap_pedestrian = cv2.VideoCapture(0)  # USB camera for pedestrian detection\n",
    "\n",
    "COUNTER_EYE = 0\n",
    "ALARM_ON = False\n",
    "\n",
    "while True:\n",
    "    ret_driver, frame_driver = cap_driver.read()\n",
    "    if ret_driver:\n",
    "        gray_driver = cv2.cvtColor(frame_driver, cv2.COLOR_BGR2GRAY)\n",
    "        rects = detector(gray_driver, 0)\n",
    "        for rect in rects:\n",
    "            shape = predictor(gray_driver, rect)\n",
    "            shape = face_utils.shape_to_np(shape)\n",
    "            leftEye = shape[lStart:lEnd]\n",
    "            rightEye = shape[rStart:rEnd]\n",
    "            leftEAR = eye_aspect_ratio(leftEye)\n",
    "            rightEAR = eye_aspect_ratio(rightEye)\n",
    "            ear = (leftEAR + rightEAR) / 2.0\n",
    "            (x, y, w, h) = face_utils.rect_to_bb(rect)\n",
    "            cv2.rectangle(frame_driver, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "            if ear < EYE_AR_THRESH:\n",
    "                COUNTER_EYE += 1\n",
    "                if COUNTER_EYE >= EYE_AR_CONSEC_FRAMES:\n",
    "                    if not ALARM_ON:\n",
    "                        ALARM_ON = True\n",
    "                        sound_alarm()\n",
    "                    cv2.putText(frame_driver, \"DROWSINESS ALERT!\", (10, 30),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "            else:\n",
    "                COUNTER_EYE = 0\n",
    "                ALARM_ON = False\n",
    "            face_roi = frame_driver[y:y + h, x:x + w]\n",
    "            try:\n",
    "                result = DeepFace.analyze(face_roi, actions=['emotion'], enforce_detection=False)\n",
    "                if isinstance(result, list):\n",
    "                    result = result[0]\n",
    "                emotion = result['dominant_emotion']\n",
    "                if emotion == \"angry\":\n",
    "                    sound_alarm()\n",
    "                    display_text = \"Aggressive Driving\"\n",
    "                else:\n",
    "                    display_text = emotion\n",
    "                cv2.putText(frame_driver, display_text, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 0, 255), 2)\n",
    "            except:\n",
    "                cv2.putText(frame_driver, \"Emotion Detection Error\", (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "        cv2.imshow('Driver Monitoring System', frame_driver)\n",
    "    ret_pedestrian, frame_pedestrian = cap_pedestrian.read()\n",
    "    if ret_pedestrian:\n",
    "        results = model(frame_pedestrian)\n",
    "        results.render()\n",
    "        df = results.pandas().xywh[0]  \n",
    "        for _, row in df.iterrows():\n",
    "            if row['name'] == 'person':\n",
    "                print(\"Pedestrian Detected!\")\n",
    "                sound_alarm()\n",
    "        cv2.imshow('Pedestrian Detection', frame_pedestrian)\n",
    "    # Press 'q' to exit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Cleanup\n",
    "cap_driver.release()\n",
    "cap_pedestrian.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ace9d7-54cd-449a-8a2a-875c0bac0411",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
